---
title: "practical1_task2"
format: html
editor: visual
---

```{r}
library(tidyverse)
library(survival)
library(dplyr)
# install.packages("naniar")
library(naniar)
# install.packages("mice")
library(mice)
library(tidyr)
library(ggplot2)
library(reshape2)
#install.packages("survival")
library(broom)
library(stats)
library(pROC)
```

## Data Manipulation

### Data Importation

```{r}
setwd("/Users/theodruilhe/Documents/M2_D3S/scoring_project")

# Import X
file_path <- file.path("data", "our_data", "X.rds")
if (file.exists(file_path)) {
  X <- readRDS(file_path)
} else {
  print("File not found. Please check the file path.")
}

# import target_Y
file_path <- file.path("data", "our_data", "target_Y.rds")
if (file.exists(file_path)) {
  y <- readRDS(file_path)
} else {
  print("File not found. Please check the file path.")
}
```

Create an new ID for the join

```{r}
X <- X %>%
  mutate(ID = paste(gvkey, fyear, sep = "_"))
y <- y %>%
  mutate(ID = paste(gvkey, fyear, sep = "_"))
```

### Data Quality check

Function to calculate the number of duplicates

```{r}
count_duplicates <- function(data) {
  # Total number of rows in the dataset
  number_of_rows <- nrow(data)
  
  # Number of distinct IDs
  distinct_count <- data %>%
    ungroup() %>%
    summarise(total_distinct_IDs = n_distinct(ID)) %>%
    pull(total_distinct_IDs) # Extract the numeric value
  
  # Calculate and return the number of duplicates
  number_of_duplicates <- number_of_rows - distinct_count
  return(number_of_duplicates)
}
```

#### Duplicates in X

```{r}
duplicates_count_X <- count_duplicates(X)
print(duplicates_count_X)
```

```{r}
duplicates_X <- X %>%
  group_by(ID) %>%
  filter(n() > 1) %>%
  ungroup()

print(duplicates_X)
```

For the duplicates rows the values of the columns are the same but not for not for the "PE_ratio" and "market_to_book" thus we will investigate the computation this columns.

We choose to keep only the smallest value for theses variables in case of duplicates

```{r}
X <- X %>%
  group_by(ID) %>% # Group by ID to handle duplicates
  arrange(PE_ratio, market_to_book) %>% # Sort by smallest PE_ratio and market_to_book
  slice(1) %>% # Keep only the first row (smallest values)
  ungroup() # Remove grouping
```

We check for other duplicates

```{r}
duplicates_count_X <- count_duplicates(X)
print(duplicates_count_X)
```

#### Duplicates in Y

```{r}
duplicates_count_y <- count_duplicates(y)
print(duplicates_count_y)
```

```{r}
duplicates_y <- y %>%
  group_by(ID) %>%
  filter(n() > 1)

print(duplicates_y)
```

```{r}
y <- y %>%
  distinct()
```

```{r}
duplicates_count_y <- count_duplicates(y)
print(duplicates_count_y)
```

```{r}
duplicates_y <- y %>%
  group_by(ID) %>%
  filter(n() > 1)

print(duplicates_y)
```

```{r}
# Get the distinct IDs from duplicates_y
distinct_ids <- duplicates_y %>%
  distinct(ID) %>%
  pull(ID) # Extract as a vector

# View the distinct IDs
print(distinct_ids)
```

We decide to keep only the rows with Y = 1

```{r}
# Filter the original dataset to remove rows where Y = 0 for the distinct IDs
y <- y %>%
  filter(!(ID %in% distinct_ids & Y == 0))
```

```{r}
duplicates_count_y <- count_duplicates(y)
print(duplicates_count_y)
```

### Create final Data

Join the data

```{r}
# Perform the inner join
data <- y %>%
  inner_join(X, by = c("ID" = "ID"))
```

```{r}
# Delete the useless columns
data <- data %>%
  select(-fyear.y, -gvkey.y)
```

```{r}
# Rename gvkey and fyear
colnames(data)[colnames(data) == "gvkey.x"] <- "gvkey"
colnames(data)[colnames(data) == "fyear.x"] <- "fyear"
```

```{r}
# Transform Y as a binary variable
data <- data %>%
  mutate(Y = ifelse(Y == 1, 1, 0))
```

#### Check for duplicates

First we will check if we do not have duplicates in the ID column

```{r}
duplicates_count_data <- count_duplicates(data)
print(duplicates_count_data)
```

## Missing Values Processing

```{r}
# get info about the data
str(data)
```

We adopt Shumway's approach to handle extreme values, ensuring the data remains robust and well-distributed. Specifically:

1.  Values exceeding the 99th percentile of each variable are capped at the 99th percentile value.
2.  Values falling below the 1st percentile of each variable are floored at the 1st percentile value.

This technique effectively minimizes the impact of outliers while preserving the integrity of the dataset.

```{r}
# Function to truncate values at 1st and 99th percentiles
truncate_outliers <- function(column) {
  p1 <- quantile(column, 0.01, na.rm = TRUE)  # 1st percentile
  p99 <- quantile(column, 0.99, na.rm = TRUE) # 99th percentile
  column <- ifelse(column < p1, p1, column)   # Floor at 1st percentile
  column <- ifelse(column > p99, p99, column) # Cap at 99th percentile
  return(column)
}

# Apply truncation to all numeric columns except Y
data <- data %>%
  mutate(across(where(is.numeric) & !all_of("Y"), truncate_outliers))
```

```{r}
# Check for missing values and give the proportion of missing values by column
missing_values <- data %>%
  summarise(across(everything(), ~ sum(is.na(.)) / n())) %>%
  gather() %>%
  arrange(desc(value))

# Print missing values
print(missing_values)
```

```{r}
gg_miss_var(data)  # Plot missing values by variable
```

We identified three groups of variables based on their proportion of missing values:

1.  **High Missingness (\>30%)**:
    -   Variables: `inventory_turnover` and `interest_coverage`.
    -   Action: These variables have over 30% missing values and will be removed from the dataset due to their unreliability.
2.  **Moderate Missingness (\~5%)**:
    -   Variables: `book_value`, `ebitda_margin`, `free_cash_flow_to_sales`, and `net_profit_margin`.
    -   Action: These variables will be handled individually on a case-by-case basis to ensure the most appropriate imputation or processing.
3.  **Low Missingness (\<1%)**:
    -   Variables: Remaining variables with less than 1% missingness.
    -   Action: Mean imputation will be applied to fill the missing values for these variables, ensuring minimal impact on the dataset's integrity.

#### 1. High missingness

```{r}
data <- data %>%
  select(-inventory_turnover, -interest_coverage)
```

#### Distribution of missing values

We want to know in which case we are based on these kind of distributions:

**a. Univariate Missing Values:** If the same individuals have missing values for the same (d \< p) variables.

**b. Monotonic Missing Values:** If the variables can be ordered so that, when the observation (y\_{ij}) is missing for the variable (Y_j), then all the following variables for the same individual, (y\_{ik}, k \> j), are also missing.

**c. Non-Monotonic or Arbitrary Missing Values:** If the missing values are without structure, i.e., they are distributed without particular structure in the dataset.

```{r}
# Créer un dataset avec les lignes ayant au moins une valeur manquante
rows_with_na <- data %>%
  filter(if_any(everything(), is.na))

# Afficher le nouveau dataset
print(rows_with_na)
```

```{r}
# Compter le nombre distinct de colonnes avec des valeurs manquantes par gvkey
missing_values_per_gvkey <- data %>%
  group_by(gvkey) %>%
  summarise(
    distinct_missing_columns = sum(sapply(across(everything(), is.na), any))
  )
```

```{r}
missing_distribution <- missing_values_per_gvkey %>%
  group_by(distinct_missing_columns) %>% # Grouper par nombre de colonnes avec NA
  summarise(
    count_gvkey = n() # Compter le nombre de gvkey pour chaque groupe
  ) %>%
  arrange(distinct_missing_columns) # Trier par nombre de colonnes avec NA

# Afficher les résultats
print(missing_distribution)
```

For the company where the number of distinct columns with missing values is greater than 5 we will delete them

```{r}
# Supprimer les gvkey avec au moins 5 distinct_missing_columns
data <- data %>%
  left_join(missing_values_per_gvkey, by = "gvkey") %>% # Joindre avec les informations des colonnes manquantes
  filter(distinct_missing_columns < 5 | is.na(distinct_missing_columns)) %>% # Garder les gvkey avec moins de 5 colonnes manquantes
  select(-distinct_missing_columns) # Optionnel : Supprimer la colonne supplémentaire si elle n'est plus nécessaire

# Afficher le dataset filtré
print(data)
```

We will look at the remaining missing values

```{r}
# Check for missing values and give the proportion of missing values by column
missing_values <- data %>%
  summarise(across(everything(), ~ sum(is.na(.)) / n())) %>%
  gather() %>%
  arrange(desc(value))

# Print missing values
print(missing_values)
```

```{r}
gg_miss_var(data)  # Plot missing values by variable
```

#### 2. Moderate Missingness

```{r}
summary(data$book_value)
```

Feature correlation analysis to asses the relevance of imputation

```{r}
numeric_data <- data %>% select(where(is.numeric))

correlations <- numeric_data %>%
  summarise(across(everything(), ~ cor(.x, numeric_data$book_value, use = "complete.obs")))

correlation_df <- tibble(variable = colnames(numeric_data), correlation = as.numeric(correlations))
```

Heatmap

```{r}
correlation_matrix <- cor(numeric_data, use = "complete.obs")

# Transformer la matrice en un format long pour ggplot
melted_correlation <- melt(correlation_matrix)

# Heatmap des corrélations
ggplot(melted_correlation, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0, limit = c(-1, 1)) +
  theme_minimal() +
  labs(title = "Correlation Heatmap", x = "", y = "") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r}
correlation_with_book_value <- correlation_df %>%
  filter(variable != "book_value") # Exclure la corrélation avec elle-même

ggplot(correlation_with_book_value, aes(x = reorder(variable, correlation), y = correlation)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  theme_minimal() +
  labs(title = "Correlation with Book Value", x = "Variables", y = "Correlation")
```

We can see that ebitda and book value are higly correlated, we can think of use ebitda to impute missing values in book_value

```{r}
# Filtrer les lignes où book_value est NA
rows_with_na_book_value <- data %>%
  filter(is.na(book_value))

# Afficher le dataset résultant
print(rows_with_na_book_value)
```

```{r}
summary(data$free_cash_flow_to_sales)
```

```{r}
# Filter correlations excluding free_cash_flow_to_sales
correlation_with_free_cash_flow <- correlation_df %>%
  filter(variable != "free_cash_flow_to_sales") # Exclude self-correlation

# Plot correlations with free_cash_flow_to_sales
ggplot(correlation_with_free_cash_flow, aes(x = reorder(variable, correlation), y = correlation)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  theme_minimal() +
  labs(
    title = "Correlation with Free Cash Flow to Sales",
    x = "Variables",
    y = "Correlation"
  )
```

Here we affect the value of the median for all Na

```{r}
summary(data$ebitda_margin)
```

```{r}
# Filter correlations excluding ebitda_margin
correlation_with_ebitda_margin <- correlation_df %>%
  filter(variable != "ebitda_margin") # Exclude self-correlation

# Plot correlations with ebitda_margin
ggplot(correlation_with_ebitda_margin, aes(x = reorder(variable, correlation), y = correlation)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  theme_minimal() +
  labs(
    title = "Correlation with EBITDA Margin",
    x = "Variables",
    y = "Correlation"
  )
```

```{r}
summary(data$net_profit_margin)
```

#### Imputation

```{r}
# Imputation avec mice
# Paramètres :
# - method = "pmm" : Predictive Mean Matching (pour variables numériques)
# - m = 5 : Générer 5 jeux de données imputés
imputed_data <- mice(data, method = "pmm", m = 5, seed = 123)

# Résumer les imputations
summary(imputed_data)

# Extraire le premier jeu de données imputé
data <- complete(imputed_data, 1)
```

```{r}
gg_miss_var(data_cleaned)  # Plot missing values by variable
```

## Descriptive Statistics

#### Target Variable

```{r}
# Calculer les gvkey ayant au moins une fois Y = 1
gvkey_with_Y1 <- data %>%
  group_by(gvkey) %>% # Grouper par gvkey
  summarise(has_Y1 = any(Y == 1)) %>% # Vérifier si Y = 1 existe pour chaque gvkey
  summarise(proportion_Y1 = mean(has_Y1)) # Calculer la proportion

# Afficher la proportion
print(gvkey_with_Y1)
```

We have almost 3% of company that have bankrupted

## Survival analysis (Hazard Model)

Create time and status variables

```{r}
# Prepare survival data
df <- data %>%
  group_by(gvkey) %>% # Group by firm identifier
  mutate(time = fyear - min(fyear),  # Calculate time-to-event
         status = Y) %>%            # Status (1 = bankruptcy, 0 = no event)
  ungroup()
```

Fit the Cox Proportional Hazards Model (all the variables except ones define below with values goes to infinity)

```{r}
# Fit the Cox Proportional Hazards Model
cox_model <- coxph(Surv(time, status) ~ 
                     ROA + ROE + net_profit_margin + asset_turnover + 
                     debt_to_equity + debt_ratio + PE_ratio + 
                     market_to_book + operating_cash_flow_to_debt + 
                     free_cash_flow_to_sales + ebitda_margin + roic + leverage, 
                   data = df)

# View model summary
summary(cox_model)
```

### Model evaluation

The C-index measures the model’s ability to rank survival times correctly.

```{r}
# Concordance index
cox_summary <- summary(cox_model)
c_index <- cox_summary$concordance[1]  # First element of concordance contains the C-index
cat("Concordance Index (C-Index):", c_index, "\n")
```

risk scores (linear predictors) to use them for stratification or classification

```{r}
# Compute risk scores
risk_scores <- predict(cox_model, type = "risk")

# Add risk scores to the dataset
df <- df %>%
  mutate(risk_score = risk_scores)
```

If we dichotomize the risk (e.g., high vs. low risk based on the median risk score), you can compute confusion matrices.

```{r}
# Dichotomize risk scores: high risk (1) vs. low risk (0) based on the median
threshold <- median(risk_scores)
df <- df %>%
  mutate(predicted_status = ifelse(risk_score > threshold, 1, 0))

# Create confusion matrix
library(caret)
confusion <- confusionMatrix(as.factor(df$predicted_status), as.factor(df$status))
print(confusion)
```

## Logistic Model

```{r}
df <- data %>%
  group_by(gvkey) %>% # Group by firm identifier
  mutate(time = fyear - min(fyear),  # Calculate time-to-event
         status = Y) %>%            # Status (1 = bankruptcy, 0 = no event)
  ungroup()
```

```{r}
str(df)
```

```{r}
# Create a formula for the logistic regression
# Exclude non-numeric predictors like gvkey, ID, and time
predictors <- names(df)[!names(df) %in% c("gvkey", "fyear", "ID", "Y", "status")]

# Construct the formula dynamically
formula <- as.formula(paste("Y ~", paste(predictors, collapse = " + ")))

# Fit the logistic regression model
logistic_model <- glm(formula, data = df, family = binomial)

# Summarize the model
summary(logistic_model)
```

\*\*Select the dataset\*\*

```{r}
data <- df
```

**Train-Test split**

```{r}
# Step 2: Train-test split
set.seed(123)  # Set seed for reproducibility
train_index <- createDataPartition(data$Y, p = 0.7, list = FALSE)  # 70% training data
train_data <- data[train_index, ]
test_data <- data[-train_index, ]
```

**Train the model**

```{r}
# Step 3: Train the Logistic Regression Model
# Create a formula for the logistic regression
# Exclude non-numeric predictors like gvkey, ID, and time
predictors <- names(data)[!names(data) %in% c("gvkey", "fyear", "ID", "Y", "status")]

# Construct the formula dynamically
formula <- as.formula(paste("Y ~", paste(predictors, collapse = " + ")))

# Fit the logistic regression model
logistic_model <- glm(formula, data = data, family = binomial)
```

**Summarize the Model**

```{r}
# Step 4: Summarize the Model
summary(logistic_model)
```

**Evaluate the model on test data**

```{r}
# Step 5: Evaluate the Model on Test Data
# Predict probabilities for the test set
test_data <- test_data %>%
  mutate(predicted_prob = predict(logistic_model, newdata = ., type = "response"))

# Create predictions based on a cutoff (e.g., 0.5)
test_data <- test_data %>%
  mutate(predicted_class = ifelse(predicted_prob > 0.5, 1, 0))
```

**Model Evaluation Metrics**

```{r}
# Step 6: Model Evaluation Metrics

# Confusion Matrix
confusion_matrix <- table(Predicted = test_data$predicted_class, Actual = test_data$Y)
print("Confusion Matrix:")
print(confusion_matrix)

# Accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
print(paste("Accuracy:", round(accuracy, 4)))

# Precision (for the positive class)
precision <- confusion_matrix[2, 2] / sum(confusion_matrix[2, ])
print(paste("Precision:", round(precision, 4)))

# Recall (Sensitivity or True Positive Rate)
recall <- confusion_matrix[2, 2] / sum(confusion_matrix[, 2])
print(paste("Recall (Sensitivity):", round(recall, 4)))

# Specificity (True Negative Rate)
specificity <- confusion_matrix[1, 1] / sum(confusion_matrix[, 1])
print(paste("Specificity:", round(specificity, 4)))

# F1 Score
f1_score <- 2 * ((precision * recall) / (precision + recall))
print(paste("F1 Score:", round(f1_score, 4)))
```

```{r}
# ROC Curve and AUC
roc_curve <- roc(test_data$Y, test_data$predicted_prob)
print(paste("AUC:", round(auc(roc_curve), 4)))
plot(roc_curve, col = "blue", main = "ROC Curve")
```

```{r}
auc(roc_curve)
```

**Choosing the good threshold**

```{r}
# Plot sensitivity and specificity against thresholds
ggplot(roc_coords_df, aes(x = threshold)) +
  geom_line(aes(y = sensitivity, color = "Sensitivity")) +
  geom_line(aes(y = specificity, color = "Specificity")) +
  labs(title = "Sensitivity and Specificity vs Threshold",
       x = "Threshold",
       y = "Value") +
  theme_minimal() +
  scale_color_manual(name = "Metric", values = c("Sensitivity" = "blue", "Specificity" = "red"))
```

**Cross Validation to choose the threshold**

```{r}
# Define a sequence of thresholds to test
thresholds <- seq(0, 1, by = 0.01)

# Initialize a dataframe to store results
cv_results <- data.frame(threshold = thresholds, accuracy = NA, precision = NA, recall = NA, f1_score = NA)

# Perform cross-validation for each threshold
for (i in seq_along(thresholds)) {
  threshold <- thresholds[i]
  
  # Create predictions based on the current threshold
  test_data <- test_data %>%
    mutate(predicted_class = ifelse(predicted_prob > threshold, 1, 0))
  
  # Confusion Matrix
  confusion_matrix <- table(Predicted = test_data$predicted_class, Actual = test_data$Y)
  
  # Handle cases where the confusion matrix dimensions are insufficient
  tp <- ifelse("1" %in% rownames(confusion_matrix) & "1" %in% colnames(confusion_matrix), confusion_matrix["1", "1"], 0)
  fp <- ifelse("1" %in% rownames(confusion_matrix) & "0" %in% colnames(confusion_matrix), confusion_matrix["1", "0"], 0)
  fn <- ifelse("0" %in% rownames(confusion_matrix) & "1" %in% colnames(confusion_matrix), confusion_matrix["0", "1"], 0)
  tn <- ifelse("0" %in% rownames(confusion_matrix) & "0" %in% colnames(confusion_matrix), confusion_matrix["0", "0"], 0)
  
  # Calculate metrics
  accuracy <- (tp + tn) / sum(confusion_matrix)
  precision <- ifelse(tp + fp > 0, tp / (tp + fp), NA)
  recall <- ifelse(tp + fn > 0, tp / (tp + fn), NA)
  f1_score <- ifelse(!is.na(precision) & !is.na(recall) & (precision + recall > 0), 2 * ((precision * recall) / (precision + recall)), NA)
  
  # Store metrics in cv_results
  cv_results[i, ] <- c(threshold, accuracy, precision, recall, f1_score)
}

# Find the optimal threshold based on a specific metric (e.g.,recall)
optimal_threshold <- cv_results %>% filter(f1_score == max(f1_score, na.rm = TRUE))
print(optimal_threshold)

# Plot metrics against thresholds
cv_results_long <- cv_results %>%
  pivot_longer(cols = c(accuracy, precision, recall, f1_score), names_to = "metric", values_to = "value")

ggplot(cv_results_long, aes(x = threshold, y = value, color = metric)) +
  geom_line() +
  labs(title = "Metrics vs Threshold", x = "Threshold", y = "Metric Value") +
  theme_minimal()
```

```{r}
# Extract the optimal threshold
optimal_threshold_value <- optimal_threshold$threshold[1]

# Create predictions based on the optimal threshold
test_data <- test_data %>%
  mutate(predicted_class = ifelse(predicted_prob > optimal_threshold_value, 1, 0))

# Generate the confusion matrix
confusion_matrix_optimal <- table(Predicted = test_data$predicted_class, Actual = test_data$Y)

# Print the confusion matrix
print("Confusion Matrix at Optimal Threshold:")
print(confusion_matrix_optimal)
```

## Task 3:

Classic **K-fold Cross-Validation** randomly splits the data into k folds, which is appropriate for independent and identically distributed (i.i.d.) data. However, time series data like yours (structured as firm-year observations) often exhibits **temporal dependency**: outcomes in one period can depend on past periods. Random splits would violate the temporal ordering and could lead to **data leakage**, where future information influences training.

Instead, **Time Series Cross-Validation (Walk Forward Validation)** respects the temporal structure by ensuring that each training set includes only observations up to the prediction period. This method simulates a real-world scenario where only past data is available for forecasting.

```{r}
# Step 1: Prepare data
data <- data_cleaned

# Step 2: Sort data by firm (gvkey) and year (fyear) to respect temporal structure
data <- data %>% arrange(gvkey, fyear)

# Step 3: Define Time Series Cross-Validation (Walk Forward Scheme)
# Create custom indices for training and testing
time_series_cv <- function(data, n_splits) {
  indices <- list()
  n <- nrow(data)
  split_size <- floor(n / (n_splits + 1))  # Calculate split size
  
  for (i in 1:n_splits) {
    train_end <- split_size * i  # End index for training
    test_start <- train_end + 1
    test_end <- test_start + split_size - 1
    
    if (test_end > n) break  # Ensure test indices stay within range
    
    train_indices <- seq(1, train_end)
    test_indices <- seq(test_start, test_end)
    
    indices[[i]] <- list(train = train_indices, test = test_indices)
  }
  indices
}

# Create 5 splits for time series CV
n_splits <- 5
cv_indices <- time_series_cv(data, n_splits)
```

```{r}

# Step 4: Perform Walk Forward Validation
results <- list()

for (i in seq_along(cv_indices)) {
  # Get train and test data
  train_data <- data[cv_indices[[i]]$train, ]
  test_data <- data[cv_indices[[i]]$test, ]
  
  # Fit logistic regression model on training data
  logistic_model <- glm(
    Y ~ ROA + ROE + net_profit_margin + asset_turnover + 
       debt_to_equity + debt_ratio + PE_ratio + 
       market_to_book + operating_cash_flow_to_debt + 
       free_cash_flow_to_sales + ebitda_margin + roic + leverage, 
    family = binomial(link = "logit"),
    data = train_data
  )
  
  # Predict probabilities on test data
  test_data <- test_data %>%
    mutate(predicted_prob = predict(logistic_model, newdata = ., type = "response"),
           predicted_class = ifelse(predicted_prob > 0.5, 1, 0))
  
  # Evaluate model performance
  confusion_matrix <- table(test_data$predicted_class, test_data$Y)
  accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
  roc_curve <- roc(test_data$Y, test_data$predicted_prob)
  auc <- auc(roc_curve)
  
  # Store results
  results[[i]] <- list(
    fold = i,
    accuracy = accuracy,
    auc = auc,
    confusion_matrix = confusion_matrix,
    roc_curve = roc_curve
  )
}
```

```{r}
# Step 5: Summarize Results
accuracy_list <- sapply(results, function(x) x$accuracy)
auc_list <- sapply(results, function(x) x$auc)

cat("Walk Forward Validation Results:\n")
cat("Average Accuracy:", round(mean(accuracy_list), 4), "\n")
cat("Average AUC:", round(mean(auc_list), 4), "\n")
```

```{r}
# Plot the last ROC curve as an example
plot(results[[n_splits]]$roc_curve, col = "blue", main = "ROC Curve for Last Fold")
```
